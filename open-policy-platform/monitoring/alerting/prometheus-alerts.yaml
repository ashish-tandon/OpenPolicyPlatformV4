groups:
  - name: service_alerts
    interval: 30s
    rules:
      # Service availability
      - alert: ServiceDown
        expr: up{job=~".*-service"} == 0
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been down for more than 2 minutes"
          value: "{{ $value }}"
          runbook_url: "https://wiki.openpolicy.com/runbooks/service-down"
          
      # High error rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
            /
            sum(rate(http_requests_total[5m])) by (service)
          ) > 0.05
        for: 5m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "High error rate in {{ $labels.service }}"
          description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.service }}"
          value: "{{ $value | humanizePercentage }}"
          
      # High response time
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
          ) > 1
        for: 10m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "High response time in {{ $labels.service }}"
          description: "95th percentile response time is {{ $value }}s"
          value: "{{ $value | humanizeDuration }}"

  - name: resource_alerts
    interval: 30s
    rules:
      # High CPU usage
      - alert: HighCPUUsage
        expr: |
          (
            100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
          ) > 80
        for: 5m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}%"
          value: "{{ $value }}%"
          
      # High memory usage
      - alert: HighMemoryUsage
        expr: |
          (
            1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)
          ) * 100 > 85
        for: 5m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}%"
          value: "{{ $value }}%"
          
      # Disk space low
      - alert: DiskSpaceLow
        expr: |
          (
            node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"}
            / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"}
          ) * 100 < 15
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Only {{ $value }}% disk space remaining on {{ $labels.mountpoint }}"
          value: "{{ $value }}%"

  - name: database_alerts
    interval: 30s
    rules:
      # PostgreSQL connection pool exhaustion
      - alert: PostgreSQLConnectionPoolExhausted
        expr: |
          (
            pg_stat_database_numbackends{datname!~"template.*|postgres"}
            / pg_settings_max_connections
          ) > 0.8
        for: 5m
        labels:
          severity: high
          team: database
          service: database
        annotations:
          summary: "PostgreSQL connection pool near exhaustion"
          description: "Database {{ $labels.datname }} using {{ $value | humanizePercentage }} of max connections"
          value: "{{ $value | humanizePercentage }}"
          
      # Slow queries
      - alert: PostgreSQLSlowQueries
        expr: |
          avg(pg_stat_statements_mean_time_seconds{datname!~"template.*|postgres"}) by (datname) > 1
        for: 10m
        labels:
          severity: high
          team: database
          service: database
        annotations:
          summary: "Slow queries in PostgreSQL"
          description: "Average query time in {{ $labels.datname }} is {{ $value }}s"
          value: "{{ $value | humanizeDuration }}"
          query: "Check pg_stat_statements for slow queries"
          
      # Redis memory usage
      - alert: RedisMemoryHigh
        expr: |
          (
            redis_memory_used_bytes / redis_memory_max_bytes
          ) > 0.8
        for: 5m
        labels:
          severity: high
          team: database
          service: database
        annotations:
          summary: "Redis memory usage high"
          description: "Redis using {{ $value | humanizePercentage }} of max memory"
          value: "{{ $value | humanizePercentage }}"
          
      # Elasticsearch cluster health
      - alert: ElasticsearchClusterRed
        expr: elasticsearch_cluster_health_status{color="red"} == 1
        for: 5m
        labels:
          severity: critical
          team: database
          service: database
        annotations:
          summary: "Elasticsearch cluster status is RED"
          description: "Elasticsearch cluster is in RED state - data loss possible"
          value: "RED"

  - name: business_kpi_alerts
    interval: 1m
    rules:
      # Low user engagement
      - alert: LowUserEngagement
        expr: |
          (
            sum(rate(user_sessions_total{status="active"}[1h]))
            / sum(rate(user_sessions_total[1h]))
          ) < 0.5
        for: 30m
        labels:
          severity: high
          team: business
          alertname: BusinessKPI
        annotations:
          summary: "User engagement dropping"
          description: "User engagement rate is {{ $value | humanizePercentage }}"
          current: "{{ $value | humanizePercentage }}"
          threshold: "50%"
          impact: "Reduced platform usage may affect business metrics"
          
      # API usage drop
      - alert: APIUsageDrop
        expr: |
          (
            sum(rate(api_calls_total[1h])) 
            < 
            sum(rate(api_calls_total[1h] offset 24h)) * 0.7
          )
        for: 1h
        labels:
          severity: high
          team: business
          alertname: BusinessKPI
        annotations:
          summary: "Significant drop in API usage"
          description: "API usage dropped by more than 30% compared to yesterday"
          metric: "API Call Rate"
          current: "{{ $value }}"
          threshold: "70% of yesterday"
          impact: "Potential service issue or user adoption problem"
          
      # Search latency
      - alert: SearchLatencyHigh
        expr: |
          histogram_quantile(0.95,
            sum(rate(search_request_duration_seconds_bucket[5m])) by (le)
          ) > 2
        for: 15m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "Search latency is high"
          description: "95th percentile search latency is {{ $value }}s"
          value: "{{ $value | humanizeDuration }}"

  - name: security_alerts
    interval: 30s
    rules:
      # Failed login attempts
      - alert: HighFailedLoginRate
        expr: |
          sum(rate(authentication_failures_total[5m])) > 10
        for: 5m
        labels:
          severity: high
          team: security
        annotations:
          summary: "High rate of failed login attempts"
          description: "{{ $value }} failed login attempts per second"
          value: "{{ $value }}/s"
          
      # Unusual API activity
      - alert: UnusualAPIActivity
        expr: |
          (
            abs(
              sum(rate(api_calls_total[5m])) - 
              sum(rate(api_calls_total[5m] offset 1w))
            ) / sum(rate(api_calls_total[5m] offset 1w))
          ) > 5
        for: 10m
        labels:
          severity: high
          team: security
        annotations:
          summary: "Unusual API activity detected"
          description: "API activity is {{ $value }}x different from same time last week"
          value: "{{ $value }}x"

  - name: kubernetes_alerts
    interval: 30s
    rules:
      # Pod crash looping
      - alert: PodCrashLooping
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Pod {{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes"
          value: "{{ $value }}"
          
      # Deployment replica mismatch
      - alert: DeploymentReplicaMismatch
        expr: |
          kube_deployment_spec_replicas{job="kube-state-metrics"}
          !=
          kube_deployment_status_replicas_available{job="kube-state-metrics"}
        for: 15m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "Deployment {{ $labels.deployment }} replica mismatch"
          description: "Deployment {{ $labels.deployment }} has {{ $value }} replicas available, expected {{ $labels.spec_replicas }}"
          value: "{{ $value }}"
          
      # PVC near full
      - alert: PersistentVolumeNearFull
        expr: |
          (
            kubelet_volume_stats_used_bytes
            / kubelet_volume_stats_capacity_bytes
          ) > 0.8
        for: 5m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "PVC {{ $labels.persistentvolumeclaim }} near full"
          description: "PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full"
          value: "{{ $value | humanizePercentage }}"